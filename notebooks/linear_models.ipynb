{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "In this notebook we will review linear models from `scikit-learn`.\n",
    "We will :\n",
    "- learn how to fit a simple linear slope and interpret the coefficients;\n",
    "- discuss feature augmentation to fit a non-linear function;\n",
    "- use `LinearRegression` and its regularized version `Ridge` which is more\n",
    "  robust;\n",
    "- use `LogisticRegression` with `pipeline`;\n",
    "- see examples of linear separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The over-simplistic toy example\n",
    "To illustrate the main principle of linear regression, we will use a dataset\n",
    "containing information about penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains features of penguins. We will formulate the following\n",
    "problem. Observing the flipper length of a penguin, we would like to infer\n",
    "is mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "feature_names = \"Flipper Length (mm)\"\n",
    "target_name = \"Body Mass (g)\"\n",
    "\n",
    "sns.scatterplot(data=data, x=feature_names, y=target_name)\n",
    "\n",
    "# select the features of interest\n",
    "X = data[[feature_names]].dropna()\n",
    "y = data[target_name].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In this problem, the mass of a penguin is our target. It is a continuous\n",
    "variable that roughly vary between 2700 g and 6300 g. Thus, this is a\n",
    "regression problem (in contrast to classification). We also see that there is\n",
    "almost linear relationship between the body mass of the penguin and the\n",
    "flipper length. Longer is the flipper, heavier is the penguin.\n",
    "\n",
    "Thus, we could come with a simple rule that given a length of the flipper\n",
    "we could compute the body mass of a penguin using a linear relationship of\n",
    "of the form `y = a * x + b` where `a` and `b` are the 2 parameters of our\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_flipper_mass(\n",
    "    flipper_length, weight_flipper_length, intercept_body_mass\n",
    "):\n",
    "    \"\"\"Linear model of the form y = a * x + b\"\"\"\n",
    "    body_mass = weight_flipper_length * flipper_length + intercept_body_mass\n",
    "    return body_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model that we define, we can check which body mass values we would\n",
    "predict for a large range of flipper length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass,\n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"Compute and plot the prediction.\"\"\"\n",
    "    inferred_body_mass = linear_model_flipper_mass(\n",
    "        flipper_length_range,\n",
    "        weight_flipper_length=weight_flipper_length,\n",
    "        intercept_body_mass=intercept_body_mass,\n",
    "    )\n",
    "\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    sns.scatterplot(data=data, x=feature_names, y=target_name, ax=ax)\n",
    "    ax.plot(\n",
    "        flipper_length_range,\n",
    "        inferred_body_mass,\n",
    "        linewidth=3,\n",
    "        label=(\n",
    "            f\"{weight_flipper_length:.2f} (g / mm) * flipper length + \"\n",
    "            f\"{intercept_body_mass:.2f} (g)\"\n",
    "        ),\n",
    "    )\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "weight_flipper_length = 45\n",
    "intercept_body_mass = -5000\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `weight_flipper_length` is a weight applied to the feature in\n",
    "order to make the inference. When this coefficient is positive, it means that\n",
    "an increase of the flipper length will induce an increase of the body mass.\n",
    "If the coefficient is negative, an increase of the flipper length will induce\n",
    "a decrease of the body mass. Graphically, this coefficient is represented by\n",
    "the slope of the curve that we draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "weight_flipper_length = -40\n",
    "intercept_body_mass = 13000\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, this coefficient has some meaningful unit. Indeed, its unit is\n",
    "g/mm. For instance, with a coefficient of 40 g/mm, it means that for an\n",
    "additional millimeter, the body weight predicted will increase of 40 g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_mass_180 = linear_model_flipper_mass(\n",
    "    flipper_length=180, weight_flipper_length=40, intercept_body_mass=0\n",
    ")\n",
    "body_mass_181 = linear_model_flipper_mass(\n",
    "    flipper_length=181, weight_flipper_length=40, intercept_body_mass=0\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The body mass for a flipper length of 180 mm is {body_mass_180} g and \"\n",
    "    f\"{body_mass_181} g for a flipper length of 181 mm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that we have a parameter `intercept_body_mass` in our model.\n",
    "this parameter is the intercept of the curve when `x=0`. If the intercept is\n",
    "null, then the curve will be passing by the origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = 25\n",
    "intercept_body_mass = 0\n",
    "\n",
    "flipper_length_range = np.linspace(0, X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, it will be the value intercepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = 45\n",
    "intercept_body_mass = -5000\n",
    "\n",
    "flipper_length_range = np.linspace(0, X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we understood how our model is inferring data, one should question\n",
    "on how to find the best value for the parameters. Indeed, it seems that we\n",
    "can have several model which will depend of the choice of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "for weight, intercept in zip([-40, 45, 90], [15000, -5000, -14000]):\n",
    "    plot_data_and_model(\n",
    "        flipper_length_range, weight, intercept, ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose a model, we could use a metric indicating how good our model is\n",
    "fitting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for weight, intercept in zip([-40, 45, 90], [15000, -5000, -14000]):\n",
    "    inferred_body_mass = linear_model_flipper_mass(\n",
    "        X,\n",
    "        weight_flipper_length=weight,\n",
    "        intercept_body_mass=intercept,\n",
    "    )\n",
    "    model_error = mean_squared_error(y, inferred_body_mass)\n",
    "    print(\n",
    "        f\"The following model \\n \"\n",
    "        f\"{weight:.2f} (g / mm) * flipper length + {intercept:.2f} (g) \\n\"\n",
    "        f\"has a mean squared error of: {model_error:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this problem can be solved without the need to check every\n",
    "potential parameters combinations. Indeed, this problem as a closed-form\n",
    "solution (i.e. an equation giving the parameter values) avoiding for any\n",
    "brute-force search. This strategy is implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance `linear_regression` will store the parameter values in the\n",
    "attribute `coef_` and `intercept_`. We can check which is the optimal model\n",
    "found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_flipper_length = linear_regression.coef_[0]\n",
    "intercept_body_mass = linear_regression.intercept_\n",
    "\n",
    "flipper_length_range = np.linspace(X.min(), X.max(), num=300)\n",
    "plot_data_and_model(\n",
    "    flipper_length_range, weight_flipper_length, intercept_body_mass\n",
    ")\n",
    "\n",
    "inferred_body_mass = linear_regression.predict(X)\n",
    "model_error = mean_squared_error(y, inferred_body_mass)\n",
    "print(f\"The error of the optimal model is {model_error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if your data don't have a linear relationship\n",
    "Now, we will define a new problem where the feature and the target are not\n",
    "linearly linked. For instance, we could defined `x` to be the years of\n",
    "experience (normalized) and `y` the salary (normalized). Therefore, the\n",
    "problem here would be to infer the salary given the years of experience of\n",
    "someone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data generation\n",
    "# fix the seed for reproduction\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_sample = 100\n",
    "x_max, x_min = 1.4, -1.4\n",
    "len_x = (x_max - x_min)\n",
    "x = rng.rand(n_sample) * len_x - len_x/2\n",
    "noise = rng.randn(n_sample) * .3\n",
    "y = x ** 3 - 0.5 * x ** 2 + noise\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(x, y,  color='k', s=9)\n",
    "plt.xlabel('x', size=26)\n",
    "_ = plt.ylabel('y', size=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "In this exercise, you are asked to approximate the target `y` by a linear\n",
    "function `f(x)`. i.e. find the best coefficients of the function `f` in order\n",
    "to minimize the error.\n",
    "\n",
    "Then you could compare the mean squared error of your model with the mean\n",
    "squared error of a linear model (which shall be the minimal one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    w0 = 0  # TODO: update the weight here\n",
    "    w1 = 0  # TODO: update the weight here\n",
    "    y_predict = w1 * x + w0\n",
    "    return y_predict\n",
    "\n",
    "\n",
    "# plot the slope of f\n",
    "grid = np.linspace(x_min, x_max, 300)\n",
    "plt.scatter(x, y, color='k', s=9)\n",
    "plt.plot(grid, f(grid), linewidth=3)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, f(x))\n",
    "print(f\"Mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1. by fiting a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "linear_regression = linear_model.LinearRegression()\n",
    "# X should be 2D for sklearn\n",
    "X = x.reshape((-1, 1))\n",
    "linear_regression.fit(X, y)\n",
    "\n",
    "# plot the best slope\n",
    "y_best = linear_regression.predict(grid.reshape(-1, 1))\n",
    "plt.plot(grid, y_best, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the coefficients learnt by `LinearRegression` is the best slope which\n",
    "fit the data. We can inspect those coefficients using the attributes of the\n",
    "model learnt as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"best coef: w1 = {linear_regression.coef_[0]:.2f}, \"\n",
    "    f\"best intercept: w0 = {linear_regression.intercept_:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the model learnt will not be able to handle\n",
    "the non-linearity linking `x` and `y` since it is beyond the assumption made\n",
    "when using a linear model. To obtain a better model, we have mainly 3\n",
    "solutions: (i) choose a model that natively can deal with non-linearity,\n",
    "(ii) \"augment\" features by including expert knowledge which can be used by\n",
    "the model, or (iii) use a \"kernel\" to have a locally-based decision function\n",
    "instead of a global linear decision function.\n",
    "\n",
    "Let's illustrate quickly the first point by using a decision tree regressor\n",
    "which natively can handle non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(X, y)\n",
    "y_pred = tree.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, tree.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model can handle the non-linearity. Instead having a model\n",
    "which natively can deal with non-linearity, we could modify our data: we\n",
    "could create new features, derived from the original features, using some\n",
    "expert knowledge. For instance, here we know that we have a cubic and squared\n",
    "relationship between `x` and `y` (because we generated the data). Indeed,\n",
    "we could create two new features that would add this information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([x, x ** 2, x ** 3]).T\n",
    "\n",
    "linear_regression.fit(X, y)\n",
    "\n",
    "grid_augmented = np.vstack([grid, grid ** 2, grid ** 3]).T\n",
    "y_pred = linear_regression.predict(grid_augmented)\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a linear model, we overcome the limitation of the\n",
    "model by adding the non-linearity component into the design of additional\n",
    "features. Here, we created new feature by knowing the way the target was\n",
    "generated. In practice, this is usually not the case. Instead, one is usually\n",
    "creating interaction between features with different order, at risk of\n",
    "creating a model with too much expressivity and wich might overfit. In\n",
    "scikit-learn, the `PolynomialFeatures` is a transformer to create such\n",
    "feature interactions which we could have used instead of creating new \n",
    "features ourself.\n",
    "\n",
    "\n",
    "To present the `PolynomialFeatures`, we are going to use a scikit-learn \n",
    "pipeline which will first create the new features and then fit the model.\n",
    "We will later comeback to details regarding scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=3), LinearRegression()\n",
    ")\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, model.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we saw that the `PolynomialFeatures` is actually doing the same\n",
    "operation that we did manually above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME: it might be to complex to be introduced here but it seems good in\n",
    "the flow. However, we go away from linear model.**\n",
    "\n",
    "The last possibility to make a linear model more expressive is to use\n",
    "\"kernel\". Instead of learning a weight per feature as we previously\n",
    "emphasized, a weight will be assign by sample instead. However, not all\n",
    "sample will be used. This is the base of the support vector machine\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel=\"linear\").fit(X, y)\n",
    "y_pred = svr.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, svr.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm can be modified such that it can use non-linear kernel. Then,\n",
    "it will compute interaction between samples using this non-linear\n",
    "interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(kernel=\"poly\", degree=3).fit(X, y)\n",
    "y_pred = svr.predict(grid.reshape(-1, 1))\n",
    "\n",
    "plt.plot(grid, y_pred, linewidth=3)\n",
    "plt.scatter(x, y, color=\"k\", s=9)\n",
    "plt.xlabel(\"x\", size=26)\n",
    "plt.ylabel(\"y\", size=26)\n",
    "\n",
    "mse = mean_squared_error(y, svr.predict(X))\n",
    "print(f\"Lowest mean squared error = {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, kernel can make a model more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in higher dimension\n",
    "In the previous example, we usually used only a single feature. But we\n",
    "already shown that we could add new feature to make the model more expressive\n",
    "by deriving this new features based on the original feature.\n",
    "\n",
    "Indeed, we could also use additional features which are decorrelated with the\n",
    "original feature and that could help us to predict the target.\n",
    "\n",
    "We will load a dataset reporting the median house value in California.\n",
    "The dataset in made of 8 features regarding demography and geography of the\n",
    "location and the aim is to predict the median house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the score of `LinearRegression` and `Ridge` (which is a\n",
    "regularized version of linear regression).\n",
    "\n",
    "We will evaluate our model using the mean squared error as in the previous\n",
    "example. The lower the score, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will divide our data into a training set and a validation set.\n",
    "The validation set will be used to assert the hyper-parameters selection.\n",
    "While a testing set should only be used to assert the score of our final\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    X, y, random_state=1\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, y_train_valid, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first example, we did not care about scaling our data to\n",
    "keep the original units and have better intuition. However, this is a good\n",
    "practice to scale the data such that each feature have a similar standard\n",
    "deviation. It will be even more important if the solver used by the model\n",
    "is a gradient-descent-based solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides several tools to preprocess the data. The\n",
    "`StandardScaler` transforms the data such that each feature will have a zero\n",
    "mean and a unit standard deviation.\n",
    "\n",
    "These scikit-learn estimators are known are transformer: they compute some\n",
    "statistics and store them when calling `fit`. Using these statistics, they\n",
    "transform the data when calling `transform`. Therefore, it is important to\n",
    "note that `fit` should only be called on the training data similarly to the\n",
    "classifier or regressor.\n",
    "\n",
    "In the example above, `X_train_scaled` are data scaled after computing the\n",
    "mean and standard deviation of each feature considering the training data.\n",
    "`X_test_scaled` are data scaled using the mean and standard deviation of each\n",
    "feature on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train_scaled, y_train)\n",
    "y_pred = linear_regression.predict(X_valid_scaled)\n",
    "print(\n",
    "    f\"Mean squared error on the validation set: \"\n",
    "    f\"{mean_squared_error(y_valid, y_pred):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually transforming the data by calling the transformer,\n",
    "scikit-learn provide a `Pipeline` allowing to call a sequence of\n",
    "transformer(s) followed by a regressor or a classifier. This pipeline exposed\n",
    "the same API than the regressor and classifier and will manage the call to\n",
    "`fit` and `transform` for you, avoiding any mistake with data leakage.\n",
    "\n",
    "We already presented `Pipeline` in the second notebook and we will use it\n",
    "here to combine both the scaling and the linear regression.\n",
    "\n",
    "We will call `make_pipeline()` which will create a `Pipeline` by giving as\n",
    "arguments the successive transformations to perform followed by the regressor\n",
    "model.\n",
    "\n",
    "So the two cells above become this new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_regression = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred_valid = linear_regression.predict(X_valid)\n",
    "linear_regression_score = mean_squared_error(y_valid, y_pred_valid)\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "\n",
    "print(\n",
    "    f\"Mean squared error on the validation set: \"\n",
    "    f\"{mean_squared_error(y_valid, y_pred_valid):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare this basic `LinearRegression` versus its regularized\n",
    "form `Ridge`.\n",
    "\n",
    "We will tune the parameter alpha and compare it with the `LinearRegression`\n",
    "model which is not regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), Ridge())\n",
    "\n",
    "list_alphas = np.logspace(-2, 2.1, num=40)\n",
    "list_ridge_scores = []\n",
    "for alpha in list_alphas:\n",
    "    ridge.set_params(ridge__alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_valid)\n",
    "    list_ridge_scores.append(mean_squared_error(y_valid, y_pred))\n",
    "\n",
    "plt.plot(\n",
    "    list_alphas, [linear_regression_score] * len(list_alphas), '--',\n",
    "    label='LinearRegression',\n",
    ")\n",
    "plt.plot(list_alphas, list_ridge_scores, \"+-\", label='Ridge')\n",
    "plt.xlabel('alpha (regularization strength)')\n",
    "plt.ylabel('Mean squared error (lower is better')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, just like adding salt in cooking, adding regularization in our\n",
    "model could improve its error on the validation set. But too much\n",
    "regularization, like too much salt, decrease its performance.\n",
    "\n",
    "We can see visually that the best alpha should be around 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = list_alphas[np.argmin(list_ridge_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we selected this alpha *without* using the testing set ; but \n",
    "instead by extracting a validation set which is a subset of the training\n",
    "data. This has been seen in the lesson *basic hyper parameters tuning*.\n",
    "We can finally compared the `LinearRegression` model and the best `Ridge`\n",
    "model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "ridge.set_params(ridge__alpha=alpha)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "# FIXME add explication why Ridge is not better (equivalent) than linear \n",
    "# regression here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter search could have been made using the `GridSearchCV`\n",
    "instead of manually splitting the training data and selecting the best alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge = GridSearchCV(\n",
    "    make_pipeline(StandardScaler(), Ridge()),\n",
    "    param_grid={\"ridge__alpha\": list_alphas},\n",
    ")\n",
    "ridge.fit(X_train_valid, y_train_valid)\n",
    "print(ridge.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` manage to test all possible given `alpha` value and picked\n",
    "up the best one with a cross-validation scheme. We can now compare with\n",
    "the `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "linear_regression.fit(X_train_valid, y_train_valid)\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is as well interesting to know that several regressors and classifiers\n",
    "in scikit-learn are optimized to make this parameter tuning. They usually\n",
    "finish with the term \"CV\" for \"Cross Validation\" (e.g. `RidgeCV`).\n",
    "They are more efficient than making the `GridSearchCV` by hand and you\n",
    "should use them instead.\n",
    "\n",
    "We will repeat the equivalent of the hyper-parameter search but instead of\n",
    "using a `GridSearchCV`, we will use `RidgeCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = make_pipeline(\n",
    "    StandardScaler(), RidgeCV(alphas=[.1, .5, 1, 5, 10, 50, 100])\n",
    ")\n",
    "ridge.fit(X_train_valid, y_train_valid)\n",
    "ridge[-1].alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression\")\n",
    "y_pred_test = linear_regression.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "print(\n",
    "    f\"Mean squared error on the test set: \"\n",
    "    f\"{mean_squared_error(y_test, y_pred_test):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the best parameter value is changing because the cross-validation\n",
    "between the different approach is internally different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "In regression, we saw that the target to be predicted was a continuous\n",
    "variable. In classification, this target will be discrete. (e.g. categorical)\n",
    "\n",
    "We will go back to our penguin dataset. However, this time we will try to\n",
    "predict the penguin species using the culmen information. We will also\n",
    "simplify our classification problem by selecting only 2 of the penguin\n",
    "species to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "\n",
    "# select the features of interest\n",
    "culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n",
    "target_column = \"Species\"\n",
    "\n",
    "data = data[culmen_columns + [target_column]]\n",
    "data[target_column] = data[target_column].str.split().str[0]\n",
    "data = data[data[target_column].apply(lambda x: x in (\"Adelie\", \"Chinstrap\"))]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly start by visualizing the feature distribution by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.pairplot(data=data, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can observe, that we have quite a simple problem. When the culmen\n",
    "length increase, the probability to be a Chinstrap penguin is closer to 1.\n",
    "However, the culmen length does not help at predicting the penguin specie.\n",
    "\n",
    "For the later model fitting, we will separate the target from the data and\n",
    "we will create a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data[culmen_columns], data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To visualize the separation found by our classifier, we will define an helper\n",
    "function `plot_decision_function` . In short, this function will fit our classifier and\n",
    "plot the edge of the decision function, where the probability to be an Adelie or\n",
    "Chinstrap will be equal (p=0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_function(X, y, clf, title=\"auto\", ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    feature_0_min, feature_0_max = (\n",
    "        X.iloc[:, 0].min() - 1,\n",
    "        X.iloc[:, 0].max() + 1,\n",
    "    )\n",
    "    feature_1_min, feature_1_max = (\n",
    "        X.iloc[:, 1].min() - 1,\n",
    "        X.iloc[:, 1].max() + 1,\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(feature_0_min, feature_0_max, plot_step),\n",
    "        np.arange(feature_1_min, feature_1_max, plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    sns.scatterplot(\n",
    "        data=pd.concat([X, y], axis=1),\n",
    "        x=X.columns[0],\n",
    "        y=X.columns[1],\n",
    "        hue=y.name,\n",
    "        ax=ax,\n",
    "    )\n",
    "    if title == \"auto\":\n",
    "        C = clf[-1].C if hasattr(clf[-1], \"C\") else clf[-1].C_\n",
    "        ax.set_title(f\"C={C}\\n with coef={clf[-1].coef_[0]}\")\n",
    "    else:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-penalized logistic regression\n",
    "\n",
    "The linear regression that we previously saw will predict a continuous\n",
    "output. When the target is a binary outcome, one can use the logistic\n",
    "function to model the probability. This model is known as logistic\n",
    "regression.\n",
    "\n",
    "Scikit-learn provides the class `LogisticRegression` which implement this\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(penalty=\"none\")\n",
    ")\n",
    "plot_decision_function(X_train, y_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that our decision function is represented by a line separating\n",
    "the 2 classes. Since the line is oblique, it means that we used a\n",
    "combination of both features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logistic_regression[-1].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, both coefficients are non-null.\n",
    "\n",
    "### Apply some regularization when fitting the logistic model\n",
    "\n",
    "The `LogisticRegression` model\n",
    "allows to apply regularization via the parameter `C`. It would be equivalent\n",
    "to shift from `LinearRegression` to `Ridge`. On the contrary to `Ridge`, the\n",
    "`C` parameter is the inverse of the regularization strength: a smaller `C`\n",
    "will lead to a more regularized model. We can check the effect of\n",
    "regularization on our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "for ax, C in zip(axs, [0.02, 0.1, 1]):\n",
    "    logistic_regression = make_pipeline(\n",
    "        StandardScaler(), LogisticRegression(C=C)\n",
    "    )\n",
    "    plot_decision_function(\n",
    "        X_train, y_train, logistic_regression, ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more regularized model will make the coefficients tend to 0. Since one of\n",
    "the feature is considered less important when fitting the model (lower\n",
    "coefficient magnitude), only one of the feature will be used when C is small.\n",
    "This feature is the culmen length which is in line with our first insight\n",
    "that we found when plotting the marginal feature probabilities.\n",
    "\n",
    "Just like the `RidgeCV` class which automatically find the optimal `alpha`, \n",
    "one can use `LogisticRegressionCV` to find the best `C` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), LogisticRegressionCV(Cs=[0.01, 0.1, 1, 10])\n",
    ")\n",
    "plot_decision_function(X_train, y_train, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond linear separation\n",
    "\n",
    "As we saw in regression, the linear classification model expects the data\n",
    "to be linearly separable. When this assumption does not hold, the model\n",
    "is not enough expressive to properly fit the data. One need to apply the same\n",
    "tricks than in regression: feature augmentation (using expert-knowledge\n",
    "potentially) or using method based on kernel.\n",
    "\n",
    "We will provide examples where we will use a kernel support vector machine\n",
    "to make classification on some toy-dataset where it is impossible to find a perfect linear\n",
    "separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import (\n",
    "    make_moons, make_classification, make_gaussian_quantiles,\n",
    ")\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=.13, random_state=42)\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=2,\n",
    ")\n",
    "X_gauss, y_gauss = make_gaussian_quantiles(\n",
    "    n_samples=50, n_features=2, n_classes=2, random_state=42,\n",
    ")\n",
    "\n",
    "datasets = [\n",
    "    [pd.DataFrame(X_moons, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_moons, name=\"class\")],\n",
    "    [pd.DataFrame(X_class, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_class, name=\"class\")],\n",
    "    [pd.DataFrame(X_gauss, columns=[\"Feature #0\", \"Feature #1\"]),\n",
    "     pd.Series(y_gauss, name=\"class\")],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "_, axs = plt.subplots(ncols=3, nrows=2, figsize=(12, 9))\n",
    "\n",
    "linear_model = make_pipeline(StandardScaler(), SVC(kernel=\"linear\"))\n",
    "kernel_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\"))\n",
    "\n",
    "for ax, (X, y) in zip(axs[0], datasets):\n",
    "    plot_decision_function(X, y, linear_model, title=\"Linear kernel\", ax=ax)\n",
    "for ax, (X, y) in zip(axs[1], datasets):\n",
    "    plot_decision_function(X, y, kernel_model, title=\"RBF kernel\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $R^2$ score decrease on each dataset, so we can say that each\n",
    "dataset is \"less linearly separable\" than the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main take away\n",
    "\n",
    "- `LinearRegression` find the best slope which minimize the mean squared\n",
    "  error on the train set\n",
    "- `Ridge` could be better on the test set, thanks to its regularization\n",
    "- `RidgeCV` and `LogisiticRegressionCV` find the best relugarization thanks\n",
    "  to cross validation on the training data\n",
    "- `pipeline` can be used to combinate a scaler and a model\n",
    "- If the data are not linearly separable, we shall use a more complex model\n",
    "  or use feature augmentation\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
